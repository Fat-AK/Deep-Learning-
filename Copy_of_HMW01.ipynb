{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fat-AK/Deep-Learning-/blob/main/Copy_of_HMW01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac2e112-b772-4bb6-ba84-7226ae3332e4",
      "metadata": {
        "tags": [],
        "id": "0ac2e112-b772-4bb6-ba84-7226ae3332e4"
      },
      "source": [
        "# 1. MDPs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "015553e2-0f70-4101-9bd1-fb5a9a16d576",
      "metadata": {
        "id": "015553e2-0f70-4101-9bd1-fb5a9a16d576"
      },
      "source": [
        "## 1.1 chess"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e67402-f1c8-4297-bc6d-805c28b3a818",
      "metadata": {
        "id": "a0e67402-f1c8-4297-bc6d-805c28b3a818"
      },
      "source": [
        " chess board :  3x3 grid with only the kings on the board. \n",
        " The goal of the game : checkmate the opponent's king or force the opponent to resign. The player who achieves this goal is considered the winner."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776a402b-8b56-45b9-b5b4-ef00345b1d91",
      "metadata": {
        "id": "776a402b-8b56-45b9-b5b4-ef00345b1d91"
      },
      "source": [
        "State state (s): All possible board positions, players, and any additional information. In this case, there are nine possible board positions, two players (white and black), and a turn counter.\n",
        "\n",
        "Action Space (a): All legal moves that can be made from the current board position. In this case, each player can move their king to any adjacent cell on the board, or they can choose to pass their turn.\n",
        "\n",
        "Reward Function (r): Win +1, Loss -1, and 0 for a draw or passing the turn.\n",
        "\n",
        "Transition Probability: The transition probability would represent the likelihood of moving from one board position to another after a move is made. In this case, the transition probability would be deterministic since there are only a limited number of possible moves from each board position.\n",
        "\n",
        "Discount Factor: The discount factor is a value between 0 and 1 that represents the importance of future rewards compared to immediate rewards. In chess, a discount factor of 1 can be used because the outcome of the game is determined at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac28e852-9627-4768-bb13-dfd82528436e",
      "metadata": {
        "id": "ac28e852-9627-4768-bb13-dfd82528436e"
      },
      "source": [
        "## 1.2 LunarLander"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91dcf28b-e9c1-4c7b-8736-72fa833dc67d",
      "metadata": {
        "id": "91dcf28b-e9c1-4c7b-8736-72fa833dc67d"
      },
      "source": [
        "The link does not work "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf9d726-8511-4f8e-b6fe-2f76e7a3625a",
      "metadata": {
        "id": "2bf9d726-8511-4f8e-b6fe-2f76e7a3625a"
      },
      "source": [
        "## 1.3 Model Based RL: Accessing Environment Dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44619be8-c5e7-4519-a161-03bf0a475a13",
      "metadata": {
        "id": "44619be8-c5e7-4519-a161-03bf0a475a13"
      },
      "source": [
        "### Discuss the Policy Evaluation and Policy Iteration algorithms from the lecture.They explicitly make use of the environment dynamics (p(s′, r|s, a))."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7695b866-1e45-46a6-ba05-d9696e210a54",
      "metadata": {
        "id": "7695b866-1e45-46a6-ba05-d9696e210a54"
      },
      "source": [
        "Policy evaluation is the process of calculating the value function for a given policy (π) in an MDP. The value function determines the expected return (Gt) for an agent starting in a particular state (s) and following a specific policy. The policy evaluation algorithm estimates the value function iteratively, starting from an arbitrary initial value function and updating it until it converges to the true value function.\n",
        "\n",
        "1- Initialize the value function arbitrarily\n",
        "Repeat:\n",
        "    For each state s:\n",
        "    Update the value function V(s) = Σ p(s', r|s, a) [r + γV(s')]\n",
        "    Where p(s', r|s, a) is the probability of transitioning to state s' and receiving reward r when taking action a from state s, and γ is the discount factor.\n",
        "    \n",
        "Policy Iteration:\n",
        "\n",
        "Policy iteration is an RL algorithm that alternates between policy evaluation and policy improvement to find the optimal policy for an MDP. The algorithm starts with an arbitrary policy and iteratively updates it until the optimal policy is found.\n",
        "\n",
        "The key step in improving the policy was understanding we could compute the optimal action given the current policy and state value by looking at the expected return of taking any action a in a given state and afterwards following the current policy:\n",
        "∑s′,r[p(s′,r|s,a)(r+Vπ(s′))]\n",
        "\n",
        "if we compare the expected Return of all possible different actions in a given state, we can just pick the best one to optimize the policy:\n",
        "π(s)=argmaxa[∑s′,r[p(s′,r|s,a)(r+Vπ(s′))]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8188f6-0f57-4b9f-9d7c-04f99ddee4cc",
      "metadata": {
        "id": "7f8188f6-0f57-4b9f-9d7c-04f99ddee4cc"
      },
      "source": [
        "### Explain what the environment dynamics (i.e. reward function and state transition function) are and give at least two examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f7a26a-c4e5-4ed1-be89-098974ff0a1d",
      "metadata": {
        "id": "b4f7a26a-c4e5-4ed1-be89-098974ff0a1d"
      },
      "source": [
        "The environment dynamics in a reinforcement learning problem refer to the reward function and state transition function, which determine the consequences of an agent's actions in the environment.\n",
        "\n",
        "Reward Function: The reward function maps a state and action pair to a numerical reward value that represents how good or bad that action was for the agent. The goal of the agent is to learn a policy that maximizes the cumulative reward it receives over time. A well-designed reward function is critical for guiding the agent towards desirable behavior.\n",
        "\n",
        "Example 1: Consider a self-driving car navigating through traffic. A reward function might assign positive rewards for reaching the destination safely and quickly, negative rewards for causing accidents or violating traffic laws, and intermediate rewards for following a smooth and comfortable driving style. The reward function should be designed to prioritize safety, as that is the most important objective.\n",
        "\n",
        "Example 2: In a game of chess, the reward function might assign a positive reward for winning the game, a negative reward for losing, and a zero reward for a draw. The reward function could also assign intermediate rewards for taking control of the center of the board, putting the opponent's king in check, or capturing the opponent's pieces.\n",
        "\n",
        "State Transition Function: The state transition function maps a current state and action pair to a next state. It determines how the environment evolves over time in response to the agent's actions. In many reinforcement learning problems, the state transition function is stochastic, meaning that the next state is not entirely predictable and can depend on random factors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dedacb6-9585-429c-8dcc-ec039c21b226",
      "metadata": {
        "id": "3dedacb6-9585-429c-8dcc-ec039c21b226"
      },
      "source": [
        "### Discuss: Are the environment dynamics generally known and can practically be used to solve a problem with RL?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07c004b-6141-47c7-8f5e-35b61dfe004d",
      "metadata": {
        "id": "b07c004b-6141-47c7-8f5e-35b61dfe004d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "25debfe8-3f0f-4b2e-8b1e-d9d26b3f1056",
      "metadata": {
        "id": "25debfe8-3f0f-4b2e-8b1e-d9d26b3f1056"
      },
      "source": [
        "#  2.Implementing a GridWorld"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d59e118-f457-442f-b292-55c3d7c9182f",
      "metadata": {
        "id": "7d59e118-f457-442f-b292-55c3d7c9182f"
      },
      "source": [
        "### Look up some examples of GridWorld! List at least three links or references to different GridWorlds you could find online"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92501b50-6553-4b73-bbf5-9646816d79ce",
      "metadata": {
        "id": "92501b50-6553-4b73-bbf5-9646816d79ce"
      },
      "source": [
        "1- https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4653d6e1-96f7-4073-84e4-eb7c5861133e",
      "metadata": {
        "id": "4653d6e1-96f7-4073-84e4-eb7c5861133e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}