{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fat-AK/Deep-Learning-/blob/main/Copy_of_HMW01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac2e112-b772-4bb6-ba84-7226ae3332e4",
      "metadata": {
        "tags": [],
        "id": "0ac2e112-b772-4bb6-ba84-7226ae3332e4"
      },
      "source": [
        "# 1. MDPs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "015553e2-0f70-4101-9bd1-fb5a9a16d576",
      "metadata": {
        "id": "015553e2-0f70-4101-9bd1-fb5a9a16d576"
      },
      "source": [
        "## 1.1 chess"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e67402-f1c8-4297-bc6d-805c28b3a818",
      "metadata": {
        "id": "a0e67402-f1c8-4297-bc6d-805c28b3a818"
      },
      "source": [
        " chess board :  3x3 grid with only the kings on the board. \n",
        " The goal of the game : checkmate the opponent's king or force the opponent to resign. The player who achieves this goal is considered the winner."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776a402b-8b56-45b9-b5b4-ef00345b1d91",
      "metadata": {
        "id": "776a402b-8b56-45b9-b5b4-ef00345b1d91"
      },
      "source": [
        "State state (s): All possible board positions, players, and any additional information. In this case, there are nine possible board positions, two players (white and black), and a turn counter.\n",
        "\n",
        "Action Space (a): All legal moves that can be made from the current board position. In this case, each player can move their king to any adjacent cell on the board, or they can choose to pass their turn.\n",
        "\n",
        "Reward Function (r): Win +1, Loss -1, and 0 for a draw or passing the turn.\n",
        "\n",
        "Transition Probability: The transition probability would represent the likelihood of moving from one board position to another after a move is made. In this case, the transition probability would be deterministic since there are only a limited number of possible moves from each board position.\n",
        "\n",
        "Discount Factor: The discount factor is a value between 0 and 1 that represents the importance of future rewards compared to immediate rewards. In chess, a discount factor of 1 can be used because the outcome of the game is determined at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac28e852-9627-4768-bb13-dfd82528436e",
      "metadata": {
        "id": "ac28e852-9627-4768-bb13-dfd82528436e"
      },
      "source": [
        "## 1.2 LunarLander"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91dcf28b-e9c1-4c7b-8736-72fa833dc67d",
      "metadata": {
        "id": "91dcf28b-e9c1-4c7b-8736-72fa833dc67d"
      },
      "source": [
        "The link does not work "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf9d726-8511-4f8e-b6fe-2f76e7a3625a",
      "metadata": {
        "id": "2bf9d726-8511-4f8e-b6fe-2f76e7a3625a"
      },
      "source": [
        "## 1.3 Model Based RL: Accessing Environment Dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44619be8-c5e7-4519-a161-03bf0a475a13",
      "metadata": {
        "id": "44619be8-c5e7-4519-a161-03bf0a475a13"
      },
      "source": [
        "### Discuss the Policy Evaluation and Policy Iteration algorithms from the lecture.They explicitly make use of the environment dynamics (p(s′, r|s, a))."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7695b866-1e45-46a6-ba05-d9696e210a54",
      "metadata": {
        "id": "7695b866-1e45-46a6-ba05-d9696e210a54"
      },
      "source": [
        "Policy evaluation is the process of calculating the value function for a given policy (π) in an MDP. The value function determines the expected return (Gt) for an agent starting in a particular state (s) and following a specific policy. The policy evaluation algorithm estimates the value function iteratively, starting from an arbitrary initial value function and updating it until it converges to the true value function.\n",
        "\n",
        "1- Initialize the value function arbitrarily\n",
        "Repeat:\n",
        "    For each state s:\n",
        "    Update the value function V(s) = Σ p(s', r|s, a) [r + γV(s')]\n",
        "    Where p(s', r|s, a) is the probability of transitioning to state s' and receiving reward r when taking action a from state s, and γ is the discount factor.\n",
        "    \n",
        "Policy Iteration:\n",
        "\n",
        "Policy iteration is an RL algorithm that alternates between policy evaluation and policy improvement to find the optimal policy for an MDP. The algorithm starts with an arbitrary policy and iteratively updates it until the optimal policy is found.\n",
        "\n",
        "The key step in improving the policy was understanding we could compute the optimal action given the current policy and state value by looking at the expected return of taking any action a in a given state and afterwards following the current policy:\n",
        "∑s′,r[p(s′,r|s,a)(r+Vπ(s′))]\n",
        "\n",
        "if we compare the expected Return of all possible different actions in a given state, we can just pick the best one to optimize the policy:\n",
        "π(s)=argmaxa[∑s′,r[p(s′,r|s,a)(r+Vπ(s′))]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8188f6-0f57-4b9f-9d7c-04f99ddee4cc",
      "metadata": {
        "id": "7f8188f6-0f57-4b9f-9d7c-04f99ddee4cc"
      },
      "source": [
        "### Explain what the environment dynamics (i.e. reward function and state transition function) are and give at least two examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f7a26a-c4e5-4ed1-be89-098974ff0a1d",
      "metadata": {
        "id": "b4f7a26a-c4e5-4ed1-be89-098974ff0a1d"
      },
      "source": [
        "The environment dynamics in a reinforcement learning problem refer to the reward function and state transition function, which determine the consequences of an agent's actions in the environment.\n",
        "\n",
        "Reward Function: The reward function maps a state and action pair to a numerical reward value that represents how good or bad that action was for the agent. The goal of the agent is to learn a policy that maximizes the cumulative reward it receives over time. A well-designed reward function is critical for guiding the agent towards desirable behavior.\n",
        "\n",
        "Example 1: Consider a self-driving car navigating through traffic. A reward function might assign positive rewards for reaching the destination safely and quickly, negative rewards for causing accidents or violating traffic laws, and intermediate rewards for following a smooth and comfortable driving style. The reward function should be designed to prioritize safety, as that is the most important objective.\n",
        "\n",
        "Example 2: In a game of chess, the reward function might assign a positive reward for winning the game, a negative reward for losing, and a zero reward for a draw. The reward function could also assign intermediate rewards for taking control of the center of the board, putting the opponent's king in check, or capturing the opponent's pieces.\n",
        "\n",
        "State Transition Function: The state transition function maps a current state and action pair to a next state. It determines how the environment evolves over time in response to the agent's actions. In many reinforcement learning problems, the state transition function is stochastic, meaning that the next state is not entirely predictable and can depend on random factors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dedacb6-9585-429c-8dcc-ec039c21b226",
      "metadata": {
        "id": "3dedacb6-9585-429c-8dcc-ec039c21b226"
      },
      "source": [
        "### Discuss: Are the environment dynamics generally known and can practically be used to solve a problem with RL?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "b07c004b-6141-47c7-8f5e-35b61dfe004d",
      "metadata": {
        "id": "b07c004b-6141-47c7-8f5e-35b61dfe004d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "25debfe8-3f0f-4b2e-8b1e-d9d26b3f1056",
      "metadata": {
        "id": "25debfe8-3f0f-4b2e-8b1e-d9d26b3f1056"
      },
      "source": [
        "#  2.Implementing a GridWorld"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d59e118-f457-442f-b292-55c3d7c9182f",
      "metadata": {
        "id": "7d59e118-f457-442f-b292-55c3d7c9182f"
      },
      "source": [
        "### 2.1 Look up some examples of GridWorld! List at least three links or references to different GridWorlds you could find online"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92501b50-6553-4b73-bbf5-9646816d79ce",
      "metadata": {
        "id": "92501b50-6553-4b73-bbf5-9646816d79ce"
      },
      "source": [
        "1.   https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html\n",
        "2.   https://taigame.org/game/gridworld\n",
        "3. https://strategywiki.org/wiki/Photon:_The_Ultimate_Game_on_Planet_Earth/Walkthrough\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Implementing the MDP"
      ],
      "metadata": {
        "id": "m1i1r8ejZsTJ"
      },
      "id": "m1i1r8ejZsTJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        " Implement a GridWorld Class"
      ],
      "metadata": {
        "id": "vauO3nVPa5Dr"
      },
      "id": "vauO3nVPa5Dr"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size, rewards, discount_factor):\n",
        "        self.size = size\n",
        "        self.rewards = rewards\n",
        "        self.discount_factor = discount_factor\n",
        "        \n",
        "        self.num_states = size[0] * size[1]\n",
        "        self.num_actions = 4 # up, down, left, right\n",
        "        \n",
        "        # create transition probability matrix\n",
        "        self.transition_probs = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
        "        for s in range(self.num_states):\n",
        "            for a in range(self.num_actions):\n",
        "                next_s = self._get_next_state(s, a)\n",
        "                self.transition_probs[s, a, next_s] = 1\n",
        "        \n",
        "    def _get_next_state(self, state, action):\n",
        "        x, y = state // self.size[1], state % self.size[1]\n",
        "        if action == 0: # up\n",
        "            x = max(x-1, 0)\n",
        "        elif action == 1: # down\n",
        "            x = min(x+1, self.size[0]-1)\n",
        "        elif action == 2: # left\n",
        "            y = max(y-1, 0)\n",
        "        elif action == 3: # right\n",
        "            y = min(y+1, self.size[1]-1)\n",
        "        return x * self.size[1] + y\n",
        "        \n",
        "    def get_transition_probs(self):\n",
        "        return self.transition_probs\n",
        "    \n",
        "    def get_rewards(self):\n",
        "        return self.rewards\n",
        "    \n",
        "    def get_discount_factor(self):\n",
        "        return self.discount_factor\n",
        "    \n",
        "    def get_num_states(self):\n",
        "        return self.num_states\n",
        "    \n",
        "    def get_num_actions(self):\n",
        "        return self.num_actions\n"
      ],
      "metadata": {
        "id": "SXjZJm3UaxLv"
      },
      "id": "SXjZJm3UaxLv",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, a simple 3x3 Gridworld MDP, the transition probabilities are ***deterministic***. This means that taking an action in a particular state always results in the same next state. For example, if the agent is in state 1 and takes the \"right\" action, it will transition to state 2 with probability 1. Therefore, this MDP is ***deterministic***."
      ],
      "metadata": {
        "id": "niExcW5LbXlB"
      },
      "id": "niExcW5LbXlB"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "4653d6e1-96f7-4073-84e4-eb7c5861133e",
      "metadata": {
        "id": "4653d6e1-96f7-4073-84e4-eb7c5861133e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size, rewards, discount_factor, walls=[], traps=[]):\n",
        "        self.size = size\n",
        "        self.rewards = rewards\n",
        "        self.discount_factor = discount_factor\n",
        "        self.walls = walls\n",
        "        self.traps = traps\n",
        "        \n",
        "        self.num_states = size[0] * size[1]\n",
        "        self.num_actions = 4 # up, down, left, right\n",
        "        \n",
        "        # create transition probability matrix\n",
        "        self.transition_probs = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
        "        for s in range(self.num_states):\n",
        "            for a in range(self.num_actions):\n",
        "                next_s = self._get_next_state(s, a)\n",
        "                self.transition_probs[s, a, next_s] = 1\n",
        "        \n",
        "    def _get_next_state(self, state, action):\n",
        "        x, y = state // self.size[1], state % self.size[1]\n",
        "        if action == 0: # up\n",
        "            if x > 0 and (x-1, y) not in self.walls:\n",
        "                x = x - 1\n",
        "        elif action == 1: # down\n",
        "            if x < self.size[0]-1 and (x+1, y) not in self.walls:\n",
        "                x = x + 1\n",
        "        elif action == 2: # left\n",
        "            if y > 0 and (x, y-1) not in self.walls:\n",
        "                y = y - 1\n",
        "        elif action == 3: # right\n",
        "            if y < self.size[1]-1 and (x, y+1) not in self.walls:\n",
        "                y = y + 1\n",
        "                \n",
        "        next_state = x * self.size[1] + y\n",
        "        \n",
        "        # check for traps\n",
        "        if next_state in self.traps:\n",
        "            return 0 # reset to initial state\n",
        "        else:\n",
        "            return next_state\n",
        "        \n",
        "    def get_transition_probs(self):\n",
        "        return self.transition_probs\n",
        "    \n",
        "    def get_rewards(self):\n",
        "        return self.rewards\n",
        "    \n",
        "    def get_discount_factor(self):\n",
        "        return self.discount_factor\n",
        "    \n",
        "    def get_num_states(self):\n",
        "        return self.num_states\n",
        "    \n",
        "    def get_num_actions(self):\n",
        "        return self.num_actions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3  Implementing a policy "
      ],
      "metadata": {
        "id": "1oRE0Ev2ewT6"
      },
      "id": "1oRE0Ev2ewT6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Implement the basic agent\n"
      ],
      "metadata": {
        "id": "Ym4i1NNte3qs"
      },
      "id": "Ym4i1NNte3qs"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class StochasticAgent:    # \n",
        "    def __init__(self, env, stochasticity=0.2):\n",
        "        self.env = env\n",
        "        self.stochasticity = stochasticity\n",
        "    \n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Given the current state, return an action according to a stochastic policy\n",
        "        with non-zero probability for every action at every state.\n",
        "        \"\"\"\n",
        "        action_probs = np.ones(self.env.get_num_actions()) * self.stochasticity / (self.env.get_num_actions() - 1)\n",
        "        action_probs[self.env.get_num_actions()//2] = 1 - self.stochasticity # set the probability of the action that corresponds to staying in place to 1 - stochasticity\n",
        "        \n",
        "        action = np.random.choice(self.env.get_num_actions(), p=action_probs)\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "-Fkw4exlfLLM"
      },
      "id": "-Fkw4exlfLLM",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2  Evaluate the policy "
      ],
      "metadata": {
        "id": "66OPqA1VfudJ"
      },
      "id": "66OPqA1VfudJ"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gridworld\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t3mXwPUjXsm",
        "outputId": "346e0a7c-fce6-4474-b144-20d1c693e2f6"
      },
      "id": "1t3mXwPUjXsm",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gridworld in /usr/local/lib/python3.9/dist-packages (0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gridworld) (1.22.4)\n",
            "Requirement already satisfied: arcade in /usr/local/lib/python3.9/dist-packages (from gridworld) (2.6.17)\n",
            "Requirement already satisfied: pillow~=9.3.0 in /usr/local/lib/python3.9/dist-packages (from arcade->gridworld) (9.3.0)\n",
            "Requirement already satisfied: pyglet==2.0.dev23 in /usr/local/lib/python3.9/dist-packages (from arcade->gridworld) (2.0.dev23)\n",
            "Requirement already satisfied: pytiled-parser==2.2.0 in /usr/local/lib/python3.9/dist-packages (from arcade->gridworld) (2.2.0)\n",
            "Requirement already satisfied: pymunk~=6.4.0 in /usr/local/lib/python3.9/dist-packages (from arcade->gridworld) (6.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pytiled-parser==2.2.0->arcade->gridworld) (4.5.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.9/dist-packages (from pytiled-parser==2.2.0->arcade->gridworld) (23.1.0)\n",
            "Requirement already satisfied: cffi>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from pymunk~=6.4.0->arcade->gridworld) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.15.0->pymunk~=6.4.0->arcade->gridworld) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSITqjHoj5le",
        "outputId": "aac9c666-9324-48a3-8c23-42d89c8e0e61"
      },
      "id": "oSITqjHoj5le",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python39.zip', '/usr/lib/python3.9', '/usr/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.9/dist-packages/IPython/extensions', '/root/.ipython', '/path/to/gridworld']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/path/to/gridworld\")"
      ],
      "metadata": {
        "id": "BOGiY7gWj9ew"
      },
      "id": "BOGiY7gWj9ew",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gridworld import GridWorld\n",
        "from agent import StochasticAgent\n",
        "\n",
        "# create the GridWorld and agent\n",
        "gw_size = (5, 5)\n",
        "gw_rewards = np.zeros(gw_size)\n",
        "gw_rewards[gw_size[0]-1, gw_size[1]-1] = 1 # reward at goal state\n",
        "gw_discount = 0.99\n",
        "gw = GridWorld(gw_size, gw_rewards, gw_discount)\n",
        "agent = StochasticAgent(gw.get_num_actions(), gw.get_num_states())\n",
        "\n",
        "# collect episodes\n",
        "num_episodes = 1000\n",
        "state_values = np.zeros(gw.get_num_states()) # state value function V(s)\n",
        "state_counts = np.zeros(gw.get_num_states()) # state visitation counts\n",
        "for i in range(num_episodes):\n",
        "    # initialize episode\n",
        "    state = np.random.randint(gw.get_num_states())\n",
        "    episode = []\n",
        "    done = False\n",
        "    while not done:\n",
        "        # take action according to the agent's policy\n",
        "        action = agent.act(state)\n",
        "        next_state, reward = gw.step(state, action)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "        done = (reward == 1) # reached goal state\n",
        "    # update state value function\n",
        "    returns = 0\n",
        "    for j in range(len(episode)-1, -1, -1): # iterate over episode backwards\n",
        "        state, action, reward = episode[j]\n",
        "        returns = gw.get_discount_factor() * returns + reward\n",
        "        if state_counts[state] == 0: # first visit to the state\n",
        "            state_values[state] = returns\n",
        "        else: # incremental update\n",
        "            state_values[state] = (state_values[state] * state_counts[state] + returns) / (state_counts[state] + 1)\n",
        "        state_counts[state] += 1\n",
        "\n",
        "# print state values\n",
        "print(\"State values:\")\n",
        "print(np.reshape(state_values, gw_size))\n"
      ],
      "metadata": {
        "id": "Wp691MNZjGlH"
      },
      "id": "Wp691MNZjGlH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}